---
import Docs from '../../layouts/Docs.astro';
---

<Docs title="Installation" description="How to install Clarissa - via npm, from source, or as a standalone binary">
  <h1>Installation</h1>
  <p>
    Clarissa can be installed in several ways depending on your preferences and setup.
    It supports multiple LLM providers including cloud APIs and local inference.
  </p>

  <h2>Requirements</h2>
  <ul>
    <li><a href="https://bun.sh" target="_blank" rel="noopener">Bun</a> v1.0 or later (for running from source or npm install)</li>
    <li>At least one LLM provider:
      <ul>
        <li><a href="https://openrouter.ai/keys" target="_blank" rel="noopener">OpenRouter API key</a> (100+ models)</li>
        <li><a href="https://platform.openai.com/api-keys" target="_blank" rel="noopener">OpenAI API key</a> (GPT models)</li>
        <li><a href="https://console.anthropic.com/settings/keys" target="_blank" rel="noopener">Anthropic API key</a> (Claude models)</li>
        <li>Apple Intelligence (macOS 26+ with Apple Silicon)</li>
        <li><a href="https://lmstudio.ai" target="_blank" rel="noopener">LM Studio</a> (local server)</li>
        <li>Local GGUF model (via <code>clarissa download</code>)</li>
      </ul>
    </li>
  </ul>

  <h2>From npm (Recommended)</h2>
  <p>The easiest way to install Clarissa is via npm or bun:</p>
  
  <h3>Using Bun</h3>
  <pre><code>bun install -g clarissa</code></pre>
  
  <h3>Using npm</h3>
  <pre><code>npm install -g clarissa</code></pre>

  <h2>From Source</h2>
  <p>Clone the repository and link the package locally:</p>
  <pre><code>git clone https://github.com/cameronrye/clarissa.git
cd clarissa
bun install
bun link</code></pre>
  <p>This will make the <code>clarissa</code> command available globally on your system.</p>

  <h2>Standalone Binary</h2>
  <p>
    Download a pre-built binary from the 
    <a href="https://github.com/cameronrye/clarissa/releases" target="_blank" rel="noopener">releases page</a> 
    and add it to your PATH.
  </p>
  
  <h3>Available Platforms</h3>
  <table>
    <thead>
      <tr>
        <th>Platform</th>
        <th>Binary Name</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>macOS (ARM)</td>
        <td><code>clarissa-macos-arm64</code></td>
      </tr>
      <tr>
        <td>macOS (Intel)</td>
        <td><code>clarissa-macos-x64</code></td>
      </tr>
      <tr>
        <td>Linux (x64)</td>
        <td><code>clarissa-linux-x64</code></td>
      </tr>
      <tr>
        <td>Linux (ARM)</td>
        <td><code>clarissa-linux-arm64</code></td>
      </tr>
      <tr>
        <td>Windows (x64)</td>
        <td><code>clarissa-windows-x64.exe</code></td>
      </tr>
    </tbody>
  </table>

  <h3>Example: macOS ARM</h3>
  <pre><code>chmod +x clarissa-macos-arm64
mv clarissa-macos-arm64 /usr/local/bin/clarissa</code></pre>

  <h2>Verify Installation</h2>
  <p>After installation, verify that Clarissa is working:</p>
  <pre><code>clarissa --help</code></pre>

  <h2>Quick Setup</h2>
  <p>Run the setup command to configure your API keys:</p>
  <pre><code>clarissa init</code></pre>
  <p>
    This will prompt for API keys for OpenRouter, OpenAI, and Anthropic.
    You can skip any provider you don't want to use. Clarissa will automatically
    use the best available provider based on your configuration.
  </p>

  <h2>Local Models (Optional)</h2>
  <p>For offline or privacy-sensitive use, download a local GGUF model:</p>
  <pre><code>clarissa download</code></pre>
  <p>Then set it as the active model:</p>
  <pre><code>clarissa use Qwen2.5-7B.gguf</code></pre>

  <h2>Next Steps</h2>
  <p>
    For more configuration options, see the <a href="../configuration/">configuration guide</a>.
    Or jump straight to <a href="../usage/">using Clarissa</a>.
  </p>
</Docs>

