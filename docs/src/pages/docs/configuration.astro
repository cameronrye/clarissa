---
import Docs from '../../layouts/Docs.astro';
---

<Docs title="Configuration" description="Configure Clarissa with a config file or environment variables">
  <h1>Configuration</h1>
  <p>
    Clarissa can be configured via a config file or environment variables.
    It supports multiple LLM providers including cloud APIs and local inference.
  </p>

  <h2>LLM Providers</h2>
  <p>Clarissa supports the following providers:</p>
  <ul>
    <li><strong>OpenRouter</strong> - Access to 100+ models (Claude, GPT-4, Gemini, Llama, etc.)</li>
    <li><strong>OpenAI</strong> - Direct GPT API access</li>
    <li><strong>Anthropic</strong> - Direct Claude API access</li>
    <li><strong>Apple Intelligence</strong> - On-device AI for macOS 26+ with Apple Silicon</li>
    <li><strong>LM Studio</strong> - Local inference via LM Studio desktop app</li>
    <li><strong>Local Llama</strong> - Direct GGUF model inference via node-llama-cpp</li>
  </ul>

  <h2>API Key Setup</h2>
  <p>
    Set up at least one provider. Get API keys from:
  </p>
  <ul>
    <li><a href="https://openrouter.ai/keys" target="_blank" rel="noopener">OpenRouter</a></li>
    <li><a href="https://platform.openai.com/api-keys" target="_blank" rel="noopener">OpenAI</a></li>
    <li><a href="https://console.anthropic.com/" target="_blank" rel="noopener">Anthropic</a></li>
  </ul>

  <h3>Option 1: Interactive Setup (Recommended)</h3>
  <p>Run the setup command:</p>
  <pre><code>clarissa init</code></pre>
  <p>This will prompt for API keys for each provider and create the config file automatically.</p>

  <h3>Option 2: Environment Variables (Backup)</h3>
  <p>Environment variables can be used as a backup when config file keys are not set:</p>
  <pre><code>export OPENROUTER_API_KEY=your_openrouter_key
export OPENAI_API_KEY=your_openai_key
export ANTHROPIC_API_KEY=your_anthropic_key</code></pre>

  <h2>Configuration Options</h2>
  <p>Settings are specified in the config file. Environment variables serve as fallbacks.</p>
  <table>
    <thead>
      <tr>
        <th>Config Key</th>
        <th>Env Fallback</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>openrouterApiKey</code></td>
        <td><code>OPENROUTER_API_KEY</code></td>
        <td>OpenRouter API key</td>
      </tr>
      <tr>
        <td><code>openaiApiKey</code></td>
        <td><code>OPENAI_API_KEY</code></td>
        <td>OpenAI API key</td>
      </tr>
      <tr>
        <td><code>anthropicApiKey</code></td>
        <td><code>ANTHROPIC_API_KEY</code></td>
        <td>Anthropic API key</td>
      </tr>
      <tr>
        <td><code>maxIterations</code></td>
        <td><code>MAX_ITERATIONS</code></td>
        <td>Maximum tool iterations (default: 10)</td>
      </tr>
      <tr>
        <td><code>debug</code></td>
        <td><code>DEBUG</code></td>
        <td>Enable debug logging (default: false)</td>
      </tr>
    </tbody>
  </table>

  <h2>Provider Selection</h2>
  <p>Clarissa automatically detects available providers in this priority order:</p>
  <ol>
    <li><strong>OpenRouter</strong> - If OpenRouter API key is configured</li>
    <li><strong>OpenAI</strong> - If OpenAI API key is configured</li>
    <li><strong>Anthropic</strong> - If Anthropic API key is configured</li>
    <li><strong>Apple Intelligence</strong> - If on macOS 26+ with Apple Silicon</li>
    <li><strong>LM Studio</strong> - If LM Studio is running with a model loaded</li>
    <li><strong>Local Llama</strong> - If a local model path is configured</li>
  </ol>
  <p>Switch providers at runtime with the <code>/provider</code> command. Your selection persists across sessions.</p>

  <h3>Example Config File</h3>
  <pre><code set:html={`{
  "openrouterApiKey": "sk-or-...",
  "openaiApiKey": "sk-...",
  "anthropicApiKey": "sk-ant-...",
  "maxIterations": 10,
  "debug": false,
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/dir"]
    }
  },
  "localLlama": {
    "modelPath": "~/.clarissa/models/qwen3-8b-q4_k_m.gguf",
    "gpuLayers": -1,
    "contextSize": 8192,
    "flashAttention": true
  }
}`} /></pre>

  <h3>Local Model Configuration</h3>
  <p>For direct GGUF model inference, configure <code>localLlama</code> in your config:</p>
  <table>
    <thead>
      <tr>
        <th>Property</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>modelPath</code></td>
        <td>string</td>
        <td>Path to the GGUF model file</td>
      </tr>
      <tr>
        <td><code>gpuLayers</code></td>
        <td>number</td>
        <td>Number of layers to offload to GPU (-1 for all)</td>
      </tr>
      <tr>
        <td><code>contextSize</code></td>
        <td>number</td>
        <td>Context window size (default: 8192)</td>
      </tr>
      <tr>
        <td><code>flashAttention</code></td>
        <td>boolean</td>
        <td>Enable flash attention for faster inference</td>
      </tr>
    </tbody>
  </table>
  <p>Download models with <code>clarissa download</code> to get recommended GGUF models.</p>

  <h3>MCP Servers</h3>
  <p>
    Configure MCP (Model Context Protocol) servers to extend Clarissa with additional tools.
    Servers defined in <code>mcpServers</code> are automatically loaded on startup.
  </p>
  <table>
    <thead>
      <tr>
        <th>Property</th>
        <th>Type</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>command</code></td>
        <td>string</td>
        <td>Command to run the MCP server</td>
      </tr>
      <tr>
        <td><code>args</code></td>
        <td>string[]</td>
        <td>Arguments to pass to the command</td>
      </tr>
      <tr>
        <td><code>env</code></td>
        <td>object</td>
        <td>Environment variables for the server</td>
      </tr>
    </tbody>
  </table>
  <p>Use <code>/mcp</code> to view connected servers and <code>/tools</code> to see available tools.</p>

  <h2>Available Models</h2>
  <p>
    Clarissa supports any model available through OpenRouter. Some popular options include:
  </p>
  <ul>
    <li><code>anthropic/claude-sonnet-4</code> - Claude Sonnet 4 (default)</li>
    <li><code>anthropic/claude-opus-4</code> - Claude Opus 4</li>
    <li><code>openai/gpt-4o</code> - GPT-4o</li>
    <li><code>google/gemini-2.0-flash-001</code> - Gemini 2.0 Flash</li>
    <li><code>meta-llama/llama-3.3-70b-instruct</code> - Llama 3.3 70B</li>
    <li><code>deepseek/deepseek-chat</code> - DeepSeek Chat</li>
  </ul>
  <p>
    You can switch models at runtime using the <code>/model</code> command.
    See the full list at <a href="https://openrouter.ai/models" target="_blank" rel="noopener">OpenRouter Models</a>.
  </p>

  <h2>CLI Options</h2>
  <p>Clarissa accepts command-line arguments for one-shot mode and configuration:</p>
  <table>
    <thead>
      <tr>
        <th>Option</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>-c, --continue</code></td>
        <td>Continue the last session</td>
      </tr>
      <tr>
        <td><code>-m, --model MODEL</code></td>
        <td>Use a specific model for this session</td>
      </tr>
      <tr>
        <td><code>--list-models</code></td>
        <td>List available models and exit</td>
      </tr>
      <tr>
        <td><code>--check-update</code></td>
        <td>Check for available updates</td>
      </tr>
      <tr>
        <td><code>--debug</code></td>
        <td>Enable debug output</td>
      </tr>
      <tr>
        <td><code>-h, --help</code></td>
        <td>Show help message</td>
      </tr>
      <tr>
        <td><code>-v, --version</code></td>
        <td>Show version number</td>
      </tr>
    </tbody>
  </table>

  <h2>CLI Commands</h2>
  <p>Clarissa provides standalone commands for setup and management:</p>
  <table>
    <thead>
      <tr>
        <th>Command</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>clarissa init</code></td>
        <td>Set up Clarissa with your API keys interactively</td>
      </tr>
      <tr>
        <td><code>clarissa upgrade</code></td>
        <td>Upgrade to the latest version</td>
      </tr>
      <tr>
        <td><code>clarissa providers [NAME]</code></td>
        <td>List available providers or switch to one</td>
      </tr>
      <tr>
        <td><code>clarissa download [ID]</code></td>
        <td>Download GGUF models for local inference</td>
      </tr>
      <tr>
        <td><code>clarissa models</code></td>
        <td>List downloaded local models</td>
      </tr>
      <tr>
        <td><code>clarissa use &lt;FILE&gt;</code></td>
        <td>Set a downloaded model as active for local-llama provider</td>
      </tr>
      <tr>
        <td><code>clarissa config</code></td>
        <td>View current configuration (with masked API keys)</td>
      </tr>
      <tr>
        <td><code>clarissa history</code></td>
        <td>Show one-shot query history</td>
      </tr>
      <tr>
        <td><code>clarissa app "&lt;message&gt;"</code></td>
        <td>Open the native macOS app with an optional question (Apple Intelligence)</td>
      </tr>
    </tbody>
  </table>

  <h2>Data Storage</h2>
  <p>Clarissa stores data in your home directory at <code>~/.clarissa/</code>:</p>
  <table>
    <thead>
      <tr>
        <th>Path</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code>~/.clarissa/config.json</code></td>
        <td>Configuration file (API keys, model, provider settings)</td>
      </tr>
      <tr>
        <td><code>~/.clarissa/sessions/</code></td>
        <td>Saved conversation sessions (JSON files)</td>
      </tr>
      <tr>
        <td><code>~/.clarissa/memories.json</code></td>
        <td>Persistent memories across sessions</td>
      </tr>
      <tr>
        <td><code>~/.clarissa/preferences.json</code></td>
        <td>User preferences (last provider, model, etc.)</td>
      </tr>
      <tr>
        <td><code>~/.clarissa/models/</code></td>
        <td>Downloaded GGUF models for local inference</td>
      </tr>
      <tr>
        <td><code>~/.clarissa/history.json</code></td>
        <td>One-shot query history</td>
      </tr>
      <tr>
        <td><code>~/.clarissa/update-check.json</code></td>
        <td>Update check cache (24-hour interval)</td>
      </tr>
    </tbody>
  </table>

  <h2>Debug Mode</h2>
  <p>Enable debug mode to see detailed logging of API calls and tool executions:</p>
  <pre><code>DEBUG=true clarissa</code></pre>
  <p>Or use the CLI flag:</p>
  <pre><code>clarissa --debug</code></pre>

  <h2>Next Steps</h2>
  <p>
    Now that you're configured, learn about the <a href="../tools/">built-in tools</a>
    or jump straight to <a href="../usage/">using Clarissa</a>.
  </p>
</Docs>

